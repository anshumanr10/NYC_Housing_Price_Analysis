{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daced1f5-4428-4027-bc68-14d03c102f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# OSU Energy Dataset Starter Notebook (Spark SQL)\n",
    "\n",
    "### **Data Use Notice**\n",
    "This dataset is sourced from Ohio State’s public dashboard and is being provided for use **only within the Data I/O 2026 challenge.** \n",
    "\n",
    "By participating, you agree to follow [Ohio State’s IDP policy](https://it.osu.edu/data/institutional-data-policy) and understand that this data should **not be used or shared outside of this competition.**\n",
    "\n",
    "**Instructions:**\n",
    "1. Run the **Install & Download Data** cell first.\n",
    "2. Then run the rest of the code to load CSVs into Spark SQL temporary views.\n",
    "3. Table names are based on CSV filenames, with hyphens replaced by underscores.\n",
    "4. Query a table with `spark.sql(\"SELECT * FROM <table_name>\")`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81deb621-28d4-4b9a-9dcc-cfdc0940979c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "\n",
      "Downloading core ZIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=13o_2ojFRCCqwmYMN3w3qu5fQxieXATTd\n",
      "From (redirected): https://drive.google.com/uc?id=13o_2ojFRCCqwmYMN3w3qu5fQxieXATTd&confirm=t&uuid=a2a2137c-1dd7-4de6-9d38-fe8420fb6244\n",
      "To: /tmp/energy_dataset/core_dataset.zip\n",
      "\r  0%|          | 0.00/201M [00:00<?, ?B/s]\r  1%|          | 1.05M/201M [00:00<00:22, 8.83MB/s]\r  1%|▏         | 2.62M/201M [00:00<00:17, 11.5MB/s]\r  2%|▏         | 4.19M/201M [00:00<00:14, 13.2MB/s]\r  3%|▎         | 6.29M/201M [00:00<00:12, 15.4MB/s]\r  4%|▍         | 8.91M/201M [00:00<00:10, 17.8MB/s]\r  6%|▌         | 11.5M/201M [00:00<00:09, 19.6MB/s]\r  7%|▋         | 14.7M/201M [00:00<00:08, 22.2MB/s]\r  9%|▉         | 18.9M/201M [00:00<00:06, 27.8MB/s]\r 17%|█▋        | 34.1M/201M [00:00<00:02, 64.0MB/s]\r 26%|██▌       | 51.9M/201M [00:01<00:01, 96.8MB/s]\r 33%|███▎      | 67.1M/201M [00:01<00:01, 112MB/s] \r 42%|████▏     | 84.9M/201M [00:01<00:00, 128MB/s]\r 51%|█████     | 102M/201M [00:01<00:00, 139MB/s] \r 61%|██████    | 122M/201M [00:01<00:00, 157MB/s]\r 70%|██████▉   | 141M/201M [00:01<00:00, 164MB/s]\r 79%|███████▊  | 158M/201M [00:01<00:00, 162MB/s]\r 89%|████████▊ | 178M/201M [00:01<00:00, 173MB/s]\r 99%|█████████▉| 199M/201M [00:01<00:00, 184MB/s]\r100%|██████████| 201M/201M [00:01<00:00, 104MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CSVs from core ZIP...\n",
      "  Extracting advanced_core/meter-readings-march-2025.csv\n",
      "  Extracting advanced_core/meter-readings-april-2025.csv\n",
      "  Extracting advanced_core/weather_data_hourly_2025.csv\n",
      "  Extracting advanced_core/meter-readings-jan-2025.csv\n",
      "  Extracting advanced_core/building_metadata.csv\n",
      "  Extracting advanced_core/meter-readings-feb-2025.csv\n",
      "\n",
      "Downloading bonus ZIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Hvqi5nv66m3b1aEN23NnUOBkVKQrfP5z\n",
      "From (redirected): https://drive.google.com/uc?id=1Hvqi5nv66m3b1aEN23NnUOBkVKQrfP5z&confirm=t&uuid=34c010be-e83c-4a9a-a551-0ec9ff6ebfb0\n",
      "To: /tmp/energy_dataset/bonus_dataset.zip\n",
      "\r  0%|          | 0.00/416M [00:00<?, ?B/s]\r  1%|          | 2.10M/416M [00:00<00:20, 20.2MB/s]\r  5%|▍         | 18.9M/416M [00:00<00:03, 105MB/s] \r  7%|▋         | 29.9M/416M [00:00<00:03, 105MB/s]\r 11%|█         | 45.1M/416M [00:00<00:03, 123MB/s]\r 14%|█▍        | 57.7M/416M [00:00<00:03, 105MB/s]\r 17%|█▋        | 71.3M/416M [00:00<00:03, 105MB/s]\r 21%|██        | 88.1M/416M [00:00<00:02, 122MB/s]\r 26%|██▌       | 106M/416M [00:00<00:02, 140MB/s] \r 29%|██▉       | 121M/416M [00:01<00:02, 125MB/s]\r 32%|███▏      | 134M/416M [00:01<00:02, 126MB/s]\r 35%|███▌      | 147M/416M [00:01<00:02, 123MB/s]\r 40%|████      | 168M/416M [00:01<00:01, 145MB/s]\r 44%|████▍     | 183M/416M [00:01<00:01, 143MB/s]\r 48%|████▊     | 198M/416M [00:01<00:01, 125MB/s]\r 51%|█████     | 212M/416M [00:01<00:01, 131MB/s]\r 54%|█████▍    | 226M/416M [00:01<00:01, 130MB/s]\r 58%|█████▊    | 241M/416M [00:01<00:01, 136MB/s]\r 61%|██████▏   | 255M/416M [00:02<00:01, 106MB/s]\r 64%|██████▍   | 267M/416M [00:02<00:01, 109MB/s]\r 67%|██████▋   | 279M/416M [00:02<00:01, 107MB/s]\r 70%|██████▉   | 291M/416M [00:02<00:01, 109MB/s]\r 74%|███████▍  | 309M/416M [00:02<00:00, 127MB/s]\r 78%|███████▊  | 323M/416M [00:02<00:00, 133MB/s]\r 81%|████████▏ | 338M/416M [00:02<00:00, 136MB/s]\r 85%|████████▌ | 355M/416M [00:02<00:00, 147MB/s]\r 89%|████████▉ | 371M/416M [00:02<00:00, 138MB/s]\r 93%|█████████▎| 387M/416M [00:03<00:00, 144MB/s]\r 97%|█████████▋| 402M/416M [00:03<00:00, 139MB/s]\r100%|██████████| 416M/416M [00:03<00:00, 127MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CSVs from bonus ZIP...\n",
      "  Extracting advanced_bonus/meter-readings-may-2025.csv\n",
      "  Extracting advanced_bonus/meter-readings-sept-2025.csv\n",
      "  Extracting advanced_bonus/meter-readings-nov-2025.csv\n",
      "  Extracting advanced_bonus/meter-readings-dec-2025.csv\n",
      "  Extracting advanced_bonus/meter-readings-aug-2025.csv\n",
      "  Extracting advanced_bonus/meter-readings-oct-2025.csv\n",
      "  Extracting advanced_bonus/meter-readings-june-2025.csv\n",
      "  Extracting advanced_bonus/meter-readings-july-2025.csv\n",
      "\n",
      "All extracted CSV files:\n",
      " - meter-readings-march-2025.csv\n",
      " - meter-readings-april-2025.csv\n",
      " - weather_data_hourly_2025.csv\n",
      " - meter-readings-jan-2025.csv\n",
      " - building_metadata.csv\n",
      " - meter-readings-feb-2025.csv\n",
      " - meter-readings-may-2025.csv\n",
      " - meter-readings-sept-2025.csv\n",
      " - meter-readings-nov-2025.csv\n",
      " - meter-readings-dec-2025.csv\n",
      " - meter-readings-aug-2025.csv\n",
      " - meter-readings-oct-2025.csv\n",
      " - meter-readings-june-2025.csv\n",
      " - meter-readings-july-2025.csv\n",
      "\n",
      "Loading meter-readings-march-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_march_2025, rows: 761112\n",
      "\n",
      "Loading meter-readings-april-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_april_2025, rows: 735840\n",
      "\n",
      "Loading weather_data_hourly_2025.csv into Spark SQL view...\n",
      "Temp view created: weather_data_hourly_2025, rows: 8688\n",
      "\n",
      "Loading meter-readings-jan-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_jan_2025, rows: 763344\n",
      "\n",
      "Loading building_metadata.csv into Spark SQL view...\n",
      "Temp view created: building_metadata, rows: 1287\n",
      "\n",
      "Loading meter-readings-feb-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_feb_2025, rows: 687456\n",
      "\n",
      "Loading meter-readings-may-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_may_2025, rows: 760368\n",
      "\n",
      "Loading meter-readings-sept-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_sept_2025, rows: 735840\n",
      "\n",
      "Loading meter-readings-nov-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_nov_2025, rows: 737184\n",
      "\n",
      "Loading meter-readings-dec-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_dec_2025, rows: 766320\n",
      "\n",
      "Loading meter-readings-aug-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_aug_2025, rows: 760368\n",
      "\n",
      "Loading meter-readings-oct-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_oct_2025, rows: 760368\n",
      "\n",
      "Loading meter-readings-june-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_june_2025, rows: 735840\n",
      "\n",
      "Loading meter-readings-july-2025.csv into Spark SQL view...\n",
      "Temp view created: meter_readings_july_2025, rows: 760368\n",
      "+-------+------+--------------------+--------+-----------+-------------------+-----------------+------------+-------------------+-------------------+-------------------+----------------------+-------------------+---------------------+----------------------+------------------+------------------+------------------------------+-----------------+--------------------+------------------+--------------------+----+-----+---+\n",
      "|meterid|siteid|            sitename|simscode|    utility|        readingtime|     readingvalue|readingunits|readingunitsdisplay| readingwindowstart|   readingwindowend|expectedwindowreadings|totalwindowreadings|missingwindowreadings|filteredwindowreadings|  readingwindowsum| readingwindowmean|readingwindowstandarddeviation| readingwindowmin|readingwindowmintime|  readingwindowmax|readingwindowmaxtime|year|month|day|\n",
      "+-------+------+--------------------+--------+-----------+-------------------+-----------------+------------+-------------------+-------------------+-------------------+----------------------+-------------------+---------------------+----------------------+------------------+------------------+------------------------------+-----------------+--------------------+------------------+--------------------+----+-----+---+\n",
      "| 245933| 44591|         Ackerman Rd|    NULL|ELECTRICITY|2025-03-01T05:00:00|8.109952560718714|         kWh|      Kilowatt hour|2025-03-01T05:00:00|2025-03-02T04:45:00|                    96|                 96|                    0|                     0| 789.8192832633522| 8.227284200659918|              1.21346213580414|4.721723755954686| 2025-03-01T15:15:00|11.313869431950929| 2025-03-02T04:30:00|2025|    3|  1|\n",
      "| 246104| 44060|       Hamilton Hall|    38.0|ELECTRICITY|2025-03-01T05:00:00|              0.0|         kWh|      Kilowatt hour|2025-03-01T05:00:00|2025-03-02T04:45:00|                    96|                 96|                    0|                     0|               0.0|               0.0|                           0.0|              0.0| 2025-03-01T05:00:00|               0.0| 2025-03-01T05:00:00|2025|    3|  1|\n",
      "| 301948| 44132|     Research Center|    73.0|       HEAT|2025-03-01T05:00:00|             NULL|         kWh|      Kilowatt hour|2025-03-01T05:00:00|2025-03-02T04:45:00|                    96|                  0|                   96|                     0|               0.0|              NULL|                          NULL|             NULL|                NULL|              NULL|                NULL|2025|    3|  1|\n",
      "| 246127| 44073|       Jennings Hall|    14.0|ELECTRICITY|2025-03-01T05:00:00|48.70549477792655|         kWh|      Kilowatt hour|2025-03-01T05:00:00|2025-03-02T04:45:00|                    96|                 96|                    0|                     0| 4797.303761445053|49.971914181719306|            0.9325376167888262|46.69837533212577| 2025-03-01T08:00:00|51.984483333571504| 2025-03-02T03:15:00|2025|    3|  1|\n",
      "| 247554| 44138|Schoenbaum Underg...|   251.0|       HEAT|2025-03-01T05:00:00|51.03786288094752|         kWh|      Kilowatt hour|2025-03-01T05:00:00|2025-03-02T04:45:00|                    96|                 96|                    0|                     0|2151.2375604560425| 22.40872458808377|            10.381617813927791|5.944383418076766| 2025-03-02T01:15:00| 51.25855578047376| 2025-03-01T05:30:00|2025|    3|  1|\n",
      "+-------+------+--------------------+--------+-----------+-------------------+-----------------+------------+-------------------+-------------------+-------------------+----------------------+-------------------+---------------------+----------------------+------------------+------------------+------------------------------+-----------------+--------------------+------------------+--------------------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Install gdown\n",
    "# ---------------------------\n",
    "%pip install gdown --quiet\n",
    "\n",
    "import gdown\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "WORKSPACE_USER = \"anshumanr10@gmail.com\"\n",
    "if os.environ.get(\"DATABRICKS_RUNTIME_VERSION\"):\n",
    "    workspace_folder = \"/tmp/energy_dataset\"\n",
    "else:\n",
    "    workspace_folder = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(workspace_folder, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Download Core + Bonus ZIPs\n",
    "# ---------------------------\n",
    "zip_files = {\n",
    "    \"core\": \"https://drive.google.com/uc?id=13o_2ojFRCCqwmYMN3w3qu5fQxieXATTd\",\n",
    "    \"bonus\": \"https://drive.google.com/uc?id=1Hvqi5nv66m3b1aEN23NnUOBkVKQrfP5z\"\n",
    "}\n",
    "\n",
    "extracted_csv_paths = []\n",
    "\n",
    "for name, url in zip_files.items():\n",
    "    zip_path = os.path.join(tmp_folder, f\"{name}_dataset.zip\")\n",
    "    print(f\"\\nDownloading {name} ZIP...\")\n",
    "    gdown.download(url, zip_path, quiet=False)\n",
    "    \n",
    "    print(f\"Extracting CSVs from {name} ZIP...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        for member in z.namelist():\n",
    "            if member.endswith(\".csv\") and \"__MACOSX\" not in member:\n",
    "                print(f\"  Extracting {member}\")\n",
    "                z.extract(member, tmp_folder)\n",
    "                extracted_csv_paths.append(os.path.join(tmp_folder, member))\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Print list of CSV files\n",
    "# ---------------------------\n",
    "print(\"\\nAll extracted CSV files:\")\n",
    "for csv_path in extracted_csv_paths:\n",
    "    print(f\" - {os.path.basename(csv_path)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Load CSVs into Spark SQL\n",
    "# ---------------------------\n",
    "for csv_path in extracted_csv_paths:\n",
    "    csv_name = os.path.basename(csv_path)\n",
    "    print(f\"\\nLoading {csv_name} into Spark SQL view...\")\n",
    "    \n",
    "    pdf = pd.read_csv(csv_path, encoding=\"latin1\")\n",
    "    df = spark.createDataFrame(pdf)\n",
    "    \n",
    "    view_name = os.path.splitext(csv_name)[0].replace(\"-\", \"_\")\n",
    "    df.createOrReplaceTempView(view_name)\n",
    "    \n",
    "    print(f\"Temp view created: {view_name}, rows: {df.count()}\")\n",
    "\n",
    "# Example usage\n",
    "first_view = os.path.splitext(os.path.basename(extracted_csv_paths[0]))[0].replace(\"-\", \"_\")\n",
    "spark.sql(f\"SELECT * FROM {first_view} LIMIT 5\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "OSU_Energy_Dataset_Starter_SparkSQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
